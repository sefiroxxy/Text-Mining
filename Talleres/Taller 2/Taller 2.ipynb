# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WzihaXml1jY8mcqhxspRJr0vHOZtTdUD

## Pregunta 1: Ley de Zpif

### Bibliotecas Para P1
"""

#ref https://www.geeksforgeeks.org/zipfs-law/#python-implementation-of-zipfs-law
import matplotlib.pyplot as plt
import re
import numpy as np
import pandas as pd

"""###Test/Train para uso Dataset de entrenamiento"""

# Cargar dataset - Cambia 'test' por 'train' para usar el dataset de entrenamiento
dataset_type = 'train'  # Opciones: 'test' o 'train'

if dataset_type == 'test':
    file_path = './test_sent_emo.csv'
elif dataset_type == 'train':
    file_path = './train_sent_emo.csv'

# Leer el archivo CSV con diferentes codificaciones hasta encontrar una que funcione
encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
df = None

for encoding in encodings_to_try:
    try:
        df = pd.read_csv(file_path, encoding=encoding)
        print(f"Archivo le√≠do exitosamente con codificaci√≥n: {encoding}")
        break
    except UnicodeDecodeError:
        continue

if df is None:
    raise ValueError("No se pudo leer el archivo con ninguna codificaci√≥n probada")

# Extraer el texto de la columna 'Utterance' y concatenarlo
text = ' '.join(df['Utterance'].astype(str))

# Diccionario para corregir caracteres mal codificados comunes
char_corrections = {
    '\x91': "'",  # left single quotation mark
    '\x92': "'",  # right single quotation mark
    '\x93': '"',  # left double quotation mark
    '\x94': '"',  # right double quotation mark
    '\x96': '-',  # en dash
    '\x97': '--', # em dash
    '\x85': '...', # horizontal ellipsis
    '\xa0': ' ',  # non-breaking space
    '\x80': '‚Ç¨',  # euro sign
    '\x82': ',',  # single low-9 quotation mark
    '\x83': 'f',  # latin small letter f with hook
    '\x84': '"',  # double low-9 quotation mark
    '\x86': '‚Ä†',  # dagger
    '\x87': '‚Ä°',  # double dagger
    '\x88': '^',  # modifier letter circumflex accent
    '\x89': '‚Ä∞',  # per mille sign
    '\x8a': '≈†',  # latin capital letter s with caron
    '\x8b': '<',  # single left-pointing angle quotation mark
    '\x8c': '≈í',  # latin capital ligature oe
    '\x8e': '≈Ω',  # latin capital letter z with caron
    '\x8f': '',   # control character
    '\x90': '',   # control character
    '\x95': '‚Ä¢',  # bullet
    '\x98': '~',  # small tilde
    '\x99': '‚Ñ¢',  # trade mark sign
    '\x9a': '≈°',  # latin small letter s with caron
    '\x9b': '>',  # single right-pointing angle quotation mark
    '\x9c': '≈ì',  # latin small ligature oe
    '\x9e': '≈æ',  # latin small letter z with caron
    '\x9f': '≈∏'   # latin capital letter y with diaeresis
}

# Aplicar correcciones
for bad_char, good_char in char_corrections.items():
    text = text.replace(bad_char, good_char)

# Normalizar caracteres Unicode despu√©s de las correcciones
import unicodedata
text = unicodedata.normalize('NFKD', text)

print(f"Dataset cargado: {dataset_type}")
print(f"N√∫mero de utterances: {len(df)}")
print(f"Primeras 200 caracteres del texto:")
print(text[:200] + "...")

textList = re.findall(r'\b[a-zA-Z]+\b', text)

#inicializando
textDict = {}
wordFrequency={}

#Frecuencia de palabras
for txt in textList:
  if txt in textDict.keys():
    textDict[txt]+=1
  else:
    textDict[txt]=1

print(textDict)

#Sort the word frequencies in descending order
#Ordenando por frecuencia de palabras, en orden descendente
wordFrequency = dict(
        sorted(
            textDict.items(),
            key=lambda x: x[1],
            reverse=True)
        )
print(wordFrequency)

print(wordFrequency)

plt.plot(wordFrequency.keys(), wordFrequency.values())

plt.loglog(wordFrequency.keys(), wordFrequency.values())

#Define two lists, rank and frequency
rank = []
frequency = []
init = 0

#Assign ranks based on frequencies of words
for freq in wordFrequency.values():
  init+=1
  rank.append(init)
  frequency.append(freq)

plt.plot(rank,frequency)
#plt.loglog(rank,frequency)

plt.xlabel('Rank')

plt.ylabel('Frecuencia')
plt.title("Ley de Zipf")

plt.show()

"""## Pregunta 2: Concluir a ra√≠z del gr√°fico

En los gr√°ficos se nota s√∫per claro el patr√≥n de la Ley de Zipf: hay poquitas palabras que se repiten much√≠simo (como you, the, it, to), y despu√©s la frecuencia baja rapid√≠simo hasta que la mayor√≠a de palabras aparecen muy poco. Esto quiere decir que el dataset est√° lleno de palabras funcionales y muletillas, mientras que las palabras con m√°s contenido aparecen mucho menos.

Si lo pensamos desde Text Mining, esto es lo esperado en cualquier texto: siempre hay que limpiar esas palabras comunes (stopwords) para que no opaquen a las dem√°s. Adem√°s, el gr√°fico nos muestra que lo m√°s interesante suele estar en esas palabras que aparecen poco, porque son las que ayudan a diferenciar temas o contextos. O sea, el dataset s√≠ cumple con la Ley de Zipf y justamente por eso creo que es posible aplicar t√©cnicas como TF-IDF para sacarle partido a la info m√°s relevante.
"""

# Instalaci√≥n de librer√≠as necesarias
!apt-get install -y libenchant-2-2
!pip install pandas nltk pyenchant wordcloud matplotlib seaborn ftfy unidecode scikit-learn

import pandas as pd
import nltk
import re
import string
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import enchant
from collections import Counter
import ftfy
from unidecode import unidecode
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

# Descargar recursos NLTK
nltk.download("punkt")
nltk.download("averaged_perceptron_tagger")

# ==========================================================
# Script de an√°lisis de longitud de palabras por emoci√≥n
# Dataset: columnas "Utterance" (texto) y "Emotion" (etiqueta)
#
# Incluye:
# - Limpieza de texto con ftfy y Unidecode
# - Filtrado de stopwords (scikit-learn) con palabras protegidas
# - C√°lculo de longitud promedio de palabras por emoci√≥n
# - Visualizaci√≥n con barplot
# ==========================================================

# ==========================================================
# Definir stopwords personalizadas
# ==========================================================
stopwords_to_remove = set(ENGLISH_STOP_WORDS)
protected_words = {"no","not","never","nor","oh","wow","why","what","hey"}

# ==========================================================
# Funciones de preprocesamiento y an√°lisis
# ==========================================================
def clean_text(text):
    """Corrige errores de codificaci√≥n y normaliza Unicode."""
    if pd.isna(text) or not isinstance(text, str):
        return ""
    text = ftfy.fix_text(text)       # corrige mojibake y comillas raras
    text = unidecode(text)           # convierte caracteres Unicode a ASCII
    return text

def tokenize(text):
    """Tokeniza un texto b√°sico, quitando puntuaci√≥n."""
    text = clean_text(text)
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = text.lower().split()
    return tokens

def filter_tokens(tokens):
    """Filtra stopwords, manteniendo palabras protegidas."""
    return [t for t in tokens if (t not in stopwords_to_remove or t in protected_words)]

def average_word_length(tokens):
    """Calcula longitud promedio de un conjunto de palabras."""
    if not tokens:
        return None
    return sum(len(w) for w in tokens) / len(tokens)

def analyze_emotions(csv_path):
    # Leer dataset
    df = pd.read_csv(csv_path)

    if "Utterance" not in df.columns or "Emotion" not in df.columns:
        raise ValueError("El CSV debe contener columnas 'Utterance' y 'Emotion'.")

    # Procesar cada Utterance
    df["tokens"] = df["Utterance"].apply(tokenize).apply(filter_tokens)
    df["avg_word_len"] = df["tokens"].apply(average_word_length)

    # Calcular promedio por emoci√≥n
    results = df.groupby("Emotion")["avg_word_len"].mean().reset_index()
    results = results.sort_values("avg_word_len")

    # ==========================================================
    # Visualizaci√≥n
    # ==========================================================
    plt.figure(figsize=(10, 6))
    sns.barplot(data=results, x="Emotion", y="avg_word_len", palette="viridis")
    plt.title("Longitud promedio de palabras por emoci√≥n (limpieza + stopwords filtradas)")
    plt.ylabel("Longitud promedio de palabra")
    plt.xlabel("Emoci√≥n")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    return results

# ==========================================================
# Ejecuci√≥n del an√°lisis
# ==========================================================
# ‚ö†Ô∏è Reemplaza "test_sent_emo.csv" por el nombre de tu archivo cargado.
output = analyze_emotions("./train_sent_emo.csv")

print("\nüìä Resultados de longitud promedio por emoci√≥n:")
print(output)

"""## Pregunta 3: Implementacion tres caracter√≠sticas del texto"""

# Instalaci√≥n de librer√≠as necesarias
!apt-get install -y libenchant-2-2
!pip install pandas nltk pyenchant wordcloud matplotlib seaborn ftfy unidecode scikit-learn

import pandas as pd
import nltk
import re
import string
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import enchant
from collections import Counter
import ftfy
from unidecode import unidecode
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

# Descargar recursos NLTK
nltk.download("punkt")
try:
    nltk.download("averaged_perceptron_tagger_eng")
except:
    nltk.download("averaged_perceptron_tagger")

# ==============================================
# üîß SETUP & DEPENDENCIAS
# ==============================================
!apt-get install -y libenchant-2-2
!pip install pandas nltk pyenchant wordcloud matplotlib seaborn ftfy unidecode scikit-learn spacy contractions
!python -m spacy download en_core_web_sm

# ==============================================
# üìö IMPORTS
# ==============================================
import pandas as pd
import re
import ftfy
import unidecode
import contractions
import enchant
import random
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import seaborn as sns
import spacy

# ==============================================
# ‚öôÔ∏è CONFIGURACI√ìN
# ==============================================
nlp = spacy.load("en_core_web_sm")
english_dict = enchant.Dict("en_US")

# Lista interna de palabras informales / slang
SLANG_LIST = [
    'gonna', 'wanna', 'ain‚Äôt', 'bruh', 'idk', 'smh', 'lol', 'btw', 'omg', 'thx',
    'cuz', 'lemme', 'yall', 'sup', 'yo', 'nah', 'innit', 'bro', 'sis'
]

# Stopwords protegidas
PROTECTED_WORDS = {"no", "not", "never", "nor", "oh", "wow", "why", "what", "hey"}

# Regex para tokenizaci√≥n que conserva contracciones
TOKEN_PATTERN = re.compile(r"[A-Za-z]+(?:'[A-Za-z]+)?")

# ==============================================
# üßº LIMPIEZA & NORMALIZACI√ìN
# ==============================================
def clean_text(text):
    if pd.isna(text):
        return ""
    text = ftfy.fix_text(text)  # Corrige mojibake
    text = text.replace("‚Äô", "'").replace("‚Äú", '"').replace("‚Äù", '"')  # Normaliza comillas
    text = re.sub(r"[\u200b-\u200d\uFEFF]", "", text)  # Quita invisibles
    text = unidecode.unidecode(text)  # Normaliza a ASCII
    return text

# ==============================================
# üß† TOKENIZACI√ìN + POS/NER
# ==============================================
def tokenize_and_filter(text):
    text = contractions.fix(text)  # Expande contracciones est√°ndar
    doc = nlp(text)
    tokens = []
    for token in doc:
        if token.pos_ == "PROPN":
            continue
        if token.ent_type_ in {"PERSON", "NORP"}:
            continue
        if token.is_stop and token.text.lower() not in PROTECTED_WORDS:
            continue
        if token.like_num or token.is_space:
            continue
        # Aplica regex personalizada
        if not TOKEN_PATTERN.fullmatch(token.text):
            continue
        tokens.append(token.text.lower())
    return tokens

# ==============================================
# ‚öôÔ∏è L√ìGICA DE INFORMALIDAD
# ==============================================
def is_informal(word):
    if word in SLANG_LIST:
        return True
    try:
        if not english_dict.check(word):
            return True
    except:
        return True
    return False

# ==============================================
# üöÄ AN√ÅLISIS PRINCIPAL
# ==============================================
def analyze_dataset(csv_path, text_column="Utterance"):
    df = pd.read_csv(csv_path)
    detailed_rows = []
    informal_counter = {}

    for idx, row in df.iterrows():
        original_text = str(row[text_column]) if text_column in row else ""
        cleaned = clean_text(original_text)
        tokens = tokenize_and_filter(cleaned)

        informals = [w for w in tokens if is_informal(w)]
        if informals:
            detailed_rows.append({
                "row_index": idx,
                "original_text": original_text,
                "informal_words": ", ".join(informals)
            })
            for w in informals:
                informal_counter[w] = informal_counter.get(w, 0) + 1

    detailed_df = pd.DataFrame(detailed_rows)

    # Porcentaje de registros con palabras informales
    percentage = (len(detailed_df) / len(df)) * 100 if len(df) > 0 else 0

    # Ejemplos aleatorios
    examples = detailed_df.sample(min(5, len(detailed_df))) if len(detailed_df) > 0 else pd.DataFrame()

    # Tabla de frecuencias
    freq_df = pd.DataFrame(sorted(informal_counter.items(), key=lambda x: x[1], reverse=True),
                           columns=["word", "frequency"])

    # Nube de palabras
    if informal_counter:
        wc = WordCloud(width=800, height=400, background_color="white").generate_from_frequencies(informal_counter)
        plt.figure(figsize=(10, 5))
        plt.imshow(wc, interpolation="bilinear")
        plt.axis("off")
        plt.show()

    # Guardar resultados
    detailed_df.to_csv("informal_words_detailed.csv", index=False)

    # Mostrar resultados
    print("üìä Porcentaje de registros con palabras informales:", round(percentage, 2), "%")
    print("\nüîç Ejemplos de textos con palabras informales:")
    print(examples[["row_index", "original_text", "informal_words"]])
    print("\nüìà Frecuencia de palabras informales:")
    print(freq_df.head(20))

    return percentage, detailed_df, freq_df

# ==============================================
# ‚ñ∂Ô∏è EJECUCI√ìN
# ==============================================
# ‚ö†Ô∏è Cambia 'test_sent_emo.csv' y 'Utterance' seg√∫n tu dataset
percentage, detailed_df, freq_df = analyze_dataset("./train_sent_emo.csv", text_column="Utterance")

# Mostrar DataFrame detallado directamente en Colab
detailed_df.head(20)

try:
    from google.colab import sheets
    sheet = sheets.InteractiveSheet(df=detailed_df)
except Exception:
    try:
        import pandas as pd
        sheet = pd.read_csv("InteractiveSheet.csv")
        print(sheet.head())
    except Exception as e:
        print("No se pudo abrir ni InteractiveSheet ni el CSV:", e)

# =========================
# üì¶ Instalaci√≥n en Colab
# =========================
!apt-get -y update >/dev/null
!pip -q install ftfy Unidecode spacy contractions wordcloud matplotlib scikit-learn pandas chardet

# Modelo de spaCy
import sys, subprocess
try:
    import en_core_web_sm  # noqa
except Exception:
    !python -m spacy download en_core_web_sm

# =========================
# üîß Imports
# =========================
import re
import unicodedata
import chardet
import pandas as pd
import numpy as np
import ftfy
from unidecode import unidecode
import spacy
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import contractions as contractions_lib  # ‚úÖ detector principal

# =========================
# ‚öôÔ∏è Carga de spaCy
# =========================
nlp = spacy.load("en_core_web_sm", disable=["lemmatizer"])
nlp.enable_pipe("ner")

# =========================
# üß± Conjuntos y patrones
# =========================
PROTECTED_WORDS = {"no", "not", "never", "nor", "oh", "wow", "why", "what", "hey"}
BASE_STOPWORDS = set(ENGLISH_STOP_WORDS) - PROTECTED_WORDS

# Tokens con ap√≥strofe interno (p.ej., don't, they've)
TOKEN_AP_PATTERN = re.compile(r"[A-Za-z]+(?:'[A-Za-z]+)+")
WORD_PATTERN = re.compile(r"[A-Za-z]+")

# Respaldo: sufijos t√≠picos de contracci√≥n (si `contractions` no detecta alguna)
CONTRACTION_SUFFIXES = ("n't", "'re", "'ve", "'ll", "'d", "'s", "'m")

# Invisibles y comillas/ap√≥strofes Unicode ‚Üí ASCII
INVISIBLE_CHARS_PATTERN = re.compile(r"[\u200B-\u200D\u2060\uFEFF]")
APOSTROPHES = { "\u2019": "'", "\u2018": "'", "\u02BC": "'", "\u2032": "'", "\uFF07": "'" }
QUOTES     = { "\u201C": '"', "\u201D": '"', "\u2033": '"', "\uFF02": '"' }

# =========================
# üßº Normalizaci√≥n
# =========================
def normalize_text(text: str) -> str:
    if not isinstance(text, str):
        text = "" if text is None else str(text)
    text = ftfy.fix_text(text)  # corrige mojibake/encoding
    for bad, good in {**APOSTROPHES, **QUOTES}.items():
        text = text.replace(bad, good)
    text = INVISIBLE_CHARS_PATTERN.sub("", text)
    text = unicodedata.normalize("NFKC", text)
    text = unidecode(text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

# =========================
# üî§ Tokenizaci√≥n
# =========================
def tokenize_preserve_apostrophes(text: str):
    tokens_ap = TOKEN_AP_PATTERN.findall(text)     # candidatos a contracci√≥n
    tokens_words = WORD_PATTERN.findall(text)      # palabras simples
    return tokens_ap, tokens_words

# =========================
# üß† Filtros spaCy: PROPN, PERSON, NORP
# =========================
def spacy_filter_tokens(text: str, candidate_tokens: list[str]) -> list[str]:
    if not candidate_tokens:
        return candidate_tokens
    doc = nlp(text)
    ents_text_types = {ent.text: ent.label_ for ent in doc.ents if ent.label_ in {"PERSON", "NORP"}}
    filtered = []
    for tok in candidate_tokens:
        if tok in ents_text_types:
            continue
        if any(t.text == tok and t.pos_ == "PROPN" for t in doc):
            continue
        filtered.append(tok)
    return filtered

# =========================
# üîé Detecci√≥n de contracciones
# =========================
def detect_with_contractions_lib(token: str) -> bool:
    """
    ‚úÖ Detector principal basado en la librer√≠a `contractions`.
    Estrategia: si contractions.fix(token) cambia el token, lo consideramos contracci√≥n.
    (No devolvemos la expansi√≥n porque SOLO queremos detectar, no expandir.)
    """
    fixed = contractions_lib.fix(token)
    # Comparaci√≥n case-insensitive y strip para evitar falsos positivos por casing
    return fixed.strip().lower() != token.strip().lower()

def detect_with_suffix_backup(token: str) -> bool:
    """ Respaldo simple cuando la librer√≠a no se√±aliza alg√∫n caso. """
    lower = token.lower()
    return "'" in token and lower.endswith(CONTRACTION_SUFFIXES)

def is_contraction(token: str) -> bool:
    # Primero la librer√≠a (m√°s robusta)
    if detect_with_contractions_lib(token):
        return True
    # Respaldo por sufijos t√≠picos
    return detect_with_suffix_backup(token)

# =========================
# üìà An√°lisis principal
# =========================
def analyze_contractions(csv_path: str, text_column: str, sample_examples: int = 10, encoding_hint: str | None = None):
    # --- Detecci√≥n de encoding
    if encoding_hint:
        encoding = encoding_hint
    else:
        with open(csv_path, "rb") as f:
            raw = f.read(1000000)
        guess = chardet.detect(raw)
        encoding = guess.get("encoding") or "utf-8"

    # --- Carga DataFrame
    df = pd.read_csv(csv_path, encoding=encoding)
    if text_column not in df.columns:
        raise ValueError(f"La columna '{text_column}' no existe en el CSV. Columnas: {list(df.columns)}")

    # --- Normalizaci√≥n
    df["_text_norm"] = df[text_column].astype(str).map(normalize_text)

    # --- Detecci√≥n por fila
    records = []
    rows_with_contractions = 0

    for idx, text in enumerate(df["_text_norm"].tolist()):
        tokens_ap, tokens_words = tokenize_preserve_apostrophes(text)

        # Filtrado spaCy (PERSON/NORP/PROPN) aplicado solo a candidatos con ap√≥strofe
        tokens_ap = spacy_filter_tokens(text, tokens_ap)

        # Stopwords solo para an√°lisis complementario (no afectan detecci√≥n)
        tokens_words = [t for t in tokens_words if t.lower() not in BASE_STOPWORDS]

        detected_here = []
        for tok in tokens_ap:
            if is_contraction(tok):
                detected_here.append(tok)

        if detected_here:
            rows_with_contractions += 1
            for tok in detected_here:
                records.append({
                    "row_index": idx,
                    "original_text": df[text_column].iloc[idx],
                    "normalized_text": text,
                    "contraction": tok,         # ‚ùósolo detectamos
                    "expanded": None            # ‚ùå no expandimos (se deja expl√≠citamente en None)
                })

    detailed_df = pd.DataFrame.from_records(records)
    pct_rows = (rows_with_contractions / len(df)) * 100 if len(df) else 0.0

    # --- Frecuencias
    if not detailed_df.empty:
        counts = Counter(detailed_df["contraction"].str.lower())
        freq_df = pd.DataFrame(counts.items(), columns=["contraction", "frequency"]).sort_values(
            "frequency", ascending=False
        )
    else:
        freq_df = pd.DataFrame(columns=["contraction", "frequency"])

    # --- Ejemplos
    examples_df = pd.DataFrame(columns=[text_column])
    if not detailed_df.empty:
        first_rows = (
            detailed_df.groupby("row_index")
            .first()
            .reset_index()
            .sort_values("row_index")
            .head(sample_examples)
        )
        examples_df = df.loc[first_rows["row_index"], [text_column]].reset_index(drop=True)

    # --- Salidas
    print("=======================================")
    print("üìä Resumen detecci√≥n de contracciones (solo detecci√≥n, sin expansi√≥n)")
    print("=======================================")
    print(f"Filas totales: {len(df)}")
    print(f"Filas con ‚â•1 contracci√≥n: {rows_with_contractions} ({pct_rows:.2f}%)")
    print("\nTop contracciones (si hay):")
    if not freq_df.empty:
        print(freq_df.head(20).to_string(index=False))
    else:
        print("No se detectaron contracciones.")

    # --- Nube de palabras (si hay frecuencia)
    if not freq_df.empty:
        wc = WordCloud(width=900, height=500, background_color="white")
        wc.generate_from_frequencies(dict(zip(freq_df["contraction"], freq_df["frequency"])))

        plt.figure(figsize=(10, 5))
        plt.imshow(wc, interpolation="bilinear")
        plt.axis("off")
        plt.title("Nube de palabras: contracciones m√°s frecuentes")
        plt.show()

    # --- Exportar CSV detallado
    detailed_df.to_csv("contractions_detected.csv", index=False)
    print("\nüíæ Exportado: contractions_detected.csv")

    return {
        "percentage_rows_with_contractions": pct_rows,
        "examples_df": examples_df,
        "freq_df": freq_df,
        "detailed_df": detailed_df,
    }

# =========================
# ‚ñ∂Ô∏è Ejemplo de uso
# =========================
results = analyze_contractions("./train_sent_emo.csv", text_column="Utterance")
#results = analyze_contractions("tu_archivo.csv", text_column="text")

"""## Pregunta 4: Para cada etiqueta del dataset y para cada caracter√≠stica implementada.

- ¬øQu√© puede concluir?    
- ¬øQu√©  patrones  caracterizan  cada  etiqueta  del  dataset?  
- Presente  al  menos  5 ejemplos por cada caracter√≠stica/etiqueta.







"""

# Obtener todas las etiquetas de emoci√≥n √∫nicas
emotion_labels = sorted(df['Emotion'].unique())
summary_data = []

# Analizar cada emoci√≥n
for emotion in emotion_labels:
    print(f"\n--- Analizando la emoci√≥n: '{emotion}' ---")
    emotion_df = df[df['Emotion'] == emotion].copy()

    # Preprocesamiento para longitud de palabras
    emotion_df["tokens"] = emotion_df["Utterance"].apply(clean_text).apply(tokenize).apply(filter_tokens)
    emotion_df["word_count"] = emotion_df["tokens"].apply(len)

    # ----------------------------------------------------
    # CARACTER√çSTICA 1: Longitud Promedio de Palabras
    # ----------------------------------------------------
    avg_word_count = emotion_df["word_count"].mean() if not emotion_df.empty else 0
    print(f"\n1. **Longitud Promedio de Enunciado:** {avg_word_count:.2f} palabras.")
    
    examples_longitud = pd.concat([
        emotion_df.sort_values(by="word_count").head(3),
        emotion_df.sort_values(by="word_count", ascending=False).head(2)
    ]).reset_index(drop=True)
    
    print("   **Patr√≥n:** Las oraciones cortas y directas pueden ser m√°s comunes en emociones intensas. Las emociones que requieren m√°s explicaci√≥n suelen tener enunciados m√°s largos.")
    print("   **Ejemplos:**")
    if not examples_longitud.empty:
        for _, row in examples_longitud.iterrows():
            print(f"      - ({row['word_count']} palabras) \"{row['Utterance']}\"")
    else:
        print("      - No se encontraron ejemplos.")

    # ----------------------------------------------------
    # CARACTER√çSTICA 2: Palabras Informales y Slang
    # ----------------------------------------------------
    informal_words = []
    for _, row in emotion_df.iterrows():
        text = str(row['Utterance'])
        tokens_informal = [w for w in tokenize(text) if is_informal(w)]
        if tokens_informal:
            informal_words.extend(tokens_informal)
    
    informal_count = Counter(informal_words)
    total_informal = sum(informal_count.values())
    print(f"\n2. **Frecuencia de Palabras Informales/Slang:** {total_informal} apariciones.")
    
    if informal_count:
        informal_examples = emotion_df[
            emotion_df["Utterance"].astype(str).str.contains(
                '|'.join(informal_count.keys()), case=False, na=False, regex=True
            )
        ].head(5).reset_index(drop=True)
    else:
        informal_examples = pd.DataFrame()

    print("   **Patr√≥n:** El uso de slang se asocia a conversaciones casuales, lo que sugiere una mayor prevalencia en emociones como la Alegr√≠a o Neutralidad.")
    print("   **Ejemplos:**")
    if not informal_examples.empty:
        for _, row in informal_examples.iterrows():
            print(f"      - \"{row['Utterance']}\"")
    else:
        print("      - No se encontraron ejemplos.")

    # ----------------------------------------------------
    # CARACTER√çSTICA 3: Contracciones
    # ----------------------------------------------------
    contractions_found = []
    for _, row in emotion_df.iterrows():
        text = str(row['Utterance'])
        tokens = re.findall(r"\b[A-Za-z]+(?:'[A-Za-z]+)?\b", text)
        for token in tokens:
            if is_contraction(token):
                contractions_found.append(token)
    
    contractions_count = Counter(contractions_found)
    total_contractions = sum(contractions_count.values())
    print(f"\n3. **Frecuencia de Contracciones:** {total_contractions} apariciones.")

    if contractions_count:
        contraction_examples = emotion_df[
            emotion_df["Utterance"].astype(str).str.contains(
                '|'.join(contractions_count.keys()), case=False, na=False, regex=True
            )
        ].head(5).reset_index(drop=True)
    else:
        contraction_examples = pd.DataFrame()

    print("   **Patr√≥n:** Las contracciones son un marcador de lenguaje hablado y espont√°neo, por lo que son comunes en todas las emociones, pero especialmente en di√°logos fluidos.")
    print("   **Ejemplos:**")
    if not contraction_examples.empty:
        for _, row in contraction_examples.iterrows():
            print(f"      - \"{row['Utterance']}\"")
    else:
        print("      - No se encontraron ejemplos.")
    
    summary_data.append({
        'Emoci√≥n': emotion,
        'Longitud Promedio': avg_word_count,
        'Contracciones': total_contractions,
        'Palabras Informales': total_informal,
    })

print("\n\n---")
print("### Resumen y Conclusiones Generales")
print("\nLos datos muestran que existen patrones ling√º√≠sticos que caracterizan cada emoci√≥n, aunque estos son sutiles.")
summary_df = pd.DataFrame(summary_data)
summary_df = summary_df.sort_values(by="Emoci√≥n").set_index("Emoci√≥n")
print(summary_df)

print("\n**Conclusiones a partir de los patrones:**")
print("1.  **Longitud de Enunciado:** Las emociones m√°s intensas como el **Miedo** y el **Disgusto** a menudo se expresan en enunciados cortos y concisos, como interjecciones. En contraste, la **Tristeza** o la **Neutralidad** pueden tener oraciones m√°s largas, reflejando una narrativa o una descripci√≥n m√°s elaborada.")
print("2.  **Informalidad y Contracciones:** Un alto uso de **contracciones** y **slang** es un indicador de la naturaleza informal y conversacional del dataset MELD (proveniente de la serie *Friends*). Esto es esperable en di√°logos casuales. Es probable que emociones como la **Alegr√≠a** y la **Sorpresa** muestren una mayor incidencia de este tipo de lenguaje, ya que suelen ser momentos de expresi√≥n espont√°nea.")
print("3.  **An√°lisis Conjunto:** Para un modelo de an√°lisis de sentimientos, estas caracter√≠sticas son vitales. No solo el contenido de las palabras (lo que se dice) es importante, sino tambi√©n la **forma** en que se dice. La longitud de la frase y el uso de lenguaje informal pueden actuar como indicadores de la intensidad o el contexto de la emoci√≥n.")
print("\n---")